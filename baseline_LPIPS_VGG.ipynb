{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Imports and Boilerplate\n",
    "\n",
    "import urllib.request\n",
    "\n",
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "import tqdm\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def tensor_to_numpy(tensor: torch.Tensor) -> np.ndarray:\n",
    "    tensor = tensor * 256\n",
    "    tensor[tensor > 255] = 255\n",
    "    tensor[tensor < 0] = 0\n",
    "    tensor = tensor.type(torch.uint8).permute(1, 2, 0).cpu().numpy()\n",
    "\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import glob\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "class DIV2K_valid_HR_dataset(Dataset):\n",
    "    def __init__(self, path):\n",
    "        self.path = path\n",
    "        self.img_list = sorted(glob.glob(self.path + \"/*.png\"))[:32]\n",
    "        self.tf = transforms.ToTensor()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.img_list)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.img_list[idx]\n",
    "        img = Image.open(img_path)\n",
    "        img = img.resize((512,512))\n",
    "        img_t = self.tf(img)\n",
    "        return img_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DIV2K_valid_HR_dataset(path=\"DIV2K_valid_HR\")\n",
    "dataloader = DataLoader(dataset=dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianFourierFeatureTransform(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    An implementation of Gaussian Fourier feature mapping.\n",
    "\n",
    "    \"Fourier Features Let Networks Learn High Frequency Functions in Low Dimensional Domains\":\n",
    "       https://arxiv.org/abs/2006.10739\n",
    "       https://people.eecs.berkeley.edu/~bmild/fourfeat/index.html\n",
    "\n",
    "    Given an input of size [batches, num_input_channels, width, height],\n",
    "     returns a tensor of size [batches, mapping_size*2, width, height].\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_input_channels, mapping_size=256, scale=10):\n",
    "        super().__init__()\n",
    "\n",
    "        self._num_input_channels = num_input_channels\n",
    "        self._mapping_size = mapping_size\n",
    "        self._B = torch.randn((num_input_channels, mapping_size)) * scale\n",
    "\n",
    "    def forward(self, x):\n",
    "        assert x.dim() == 4, 'Expected 4D input (got {}D input)'.format(x.dim())\n",
    "\n",
    "        batches, channels, width, height = x.shape\n",
    "\n",
    "        assert channels == self._num_input_channels,\\\n",
    "            \"Expected input to have {} channels (got {} channels)\".format(self._num_input_channels, channels)\n",
    "\n",
    "        # Make shape compatible for matmul with _B.\n",
    "        # From [B, C, W, H] to [(B*W*H), C].\n",
    "        x = x.permute(0, 2, 3, 1).reshape(batches * width * height, channels)\n",
    "\n",
    "        x = x @ self._B.to(x.device)\n",
    "\n",
    "        # From [(B*W*H), C] to [B, W, H, C]\n",
    "        x = x.view(batches, width, height, self._mapping_size)\n",
    "        # From [B, W, H, C] to [B, C, W, H]\n",
    "        x = x.permute(0, 3, 1, 2)\n",
    "\n",
    "        x = 2 * np.pi * x\n",
    "        return torch.cat([torch.sin(x), torch.cos(x)], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://cvnote.ddlee.cc/2019/09/12/psnr-ssim-python\n",
    "\n",
    "import math\n",
    "def calculate_psnr(img1, img2):\n",
    "    # img1 and img2 have range [0, 255]\n",
    "    img1 = img1.astype(np.float64)\n",
    "    img2 = img2.astype(np.float64)\n",
    "    mse = np.mean((img1 - img2)**2)\n",
    "    if mse == 0:\n",
    "        return float('inf')\n",
    "    return 20 * math.log10(255.0 / math.sqrt(mse))\n",
    "\n",
    "# https://cvnote.ddlee.cc/2019/09/12/psnr-ssim-python\n",
    "import math\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "def ssim(img1, img2):\n",
    "    C1 = (0.01 * 255)**2\n",
    "    C2 = (0.03 * 255)**2\n",
    "\n",
    "    img1 = img1.astype(np.float64)\n",
    "    img2 = img2.astype(np.float64)\n",
    "    kernel = cv2.getGaussianKernel(11, 1.5)\n",
    "    window = np.outer(kernel, kernel.transpose())\n",
    "\n",
    "    mu1 = cv2.filter2D(img1, -1, window)[5:-5, 5:-5]  # valid\n",
    "    mu2 = cv2.filter2D(img2, -1, window)[5:-5, 5:-5]\n",
    "    mu1_sq = mu1**2\n",
    "    mu2_sq = mu2**2\n",
    "    mu1_mu2 = mu1 * mu2\n",
    "    sigma1_sq = cv2.filter2D(img1**2, -1, window)[5:-5, 5:-5] - mu1_sq\n",
    "    sigma2_sq = cv2.filter2D(img2**2, -1, window)[5:-5, 5:-5] - mu2_sq\n",
    "    sigma12 = cv2.filter2D(img1 * img2, -1, window)[5:-5, 5:-5] - mu1_mu2\n",
    "\n",
    "    ssim_map = ((2 * mu1_mu2 + C1) * (2 * sigma12 + C2)) / ((mu1_sq + mu2_sq + C1) *\n",
    "                                                            (sigma1_sq + sigma2_sq + C2))\n",
    "    return ssim_map.mean()\n",
    "\n",
    "\n",
    "def calculate_ssim(img1, img2):\n",
    "    '''calculate SSIM\n",
    "    the same outputs as MATLAB's\n",
    "    img1, img2: [0, 255]\n",
    "    '''\n",
    "    if not img1.shape == img2.shape:\n",
    "        raise ValueError('Input images must have the same dimensions.')\n",
    "    if img1.ndim == 2:\n",
    "        return ssim(img1, img2)\n",
    "    elif img1.ndim == 3:\n",
    "        if img1.shape[2] == 3:\n",
    "            ssims = []\n",
    "            for i in range(3):\n",
    "                ssims.append(ssim(img1, img2))\n",
    "            return np.array(ssims).mean()\n",
    "        elif img1.shape[2] == 1:\n",
    "            return ssim(np.squeeze(img1), np.squeeze(img2))\n",
    "    else:\n",
    "        raise ValueError('Wrong input image dimensions.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/32 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/intern/anaconda3/envs/fourier/lib/python3.7/site-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and may be removed in the future, \"\n",
      "/home/intern/anaconda3/envs/fourier/lib/python3.7/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from: /home/intern/anaconda3/envs/fourier/lib/python3.7/site-packages/lpips/weights/v0.1/vgg.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/32 [06:44<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_641497/4044944943.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0mgenerated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn_alex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerated\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fourier/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fourier/lib/python3.7/site-packages/lpips/lpips.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, in0, in1, retPerLayer, normalize)\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0mval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0mval\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretPerLayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def create_new_model():\n",
    "    model = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                256,\n",
    "                256,\n",
    "                kernel_size=1,\n",
    "                padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(256),\n",
    "\n",
    "            nn.Conv2d(\n",
    "                256,\n",
    "                256,\n",
    "                kernel_size=1,\n",
    "                padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(256),\n",
    "\n",
    "            nn.Conv2d(\n",
    "                256,\n",
    "                256,\n",
    "                kernel_size=1,\n",
    "                padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(256),\n",
    "\n",
    "            nn.Conv2d(\n",
    "                256,\n",
    "                3,\n",
    "                kernel_size=1,\n",
    "                padding=0),\n",
    "            nn.Sigmoid(),\n",
    "\n",
    "        ).to(device)\n",
    "    return model\n",
    "\n",
    "psnr_dict = {}\n",
    "ssim_dict = {}\n",
    "import lpips\n",
    "# Note: this can be done outside of the training loop, since the result at this stage is unchanged during the course of training.\n",
    "for ind, img in enumerate(tqdm(dataloader)):\n",
    "    target = img.to(device)\n",
    "    # Create input pixel coordinates in the unit square. This will be the input to the model.\n",
    "    coords = np.linspace(0, 1, target.shape[2], endpoint=False)\n",
    "    xy_grid = np.stack(np.meshgrid(coords, coords), -1)\n",
    "    xy_grid = torch.tensor(xy_grid).unsqueeze(0).permute(0, 3, 1, 2).float().contiguous().to(device)\n",
    "    x = GaussianFourierFeatureTransform(2, 128, 10)(xy_grid)\n",
    "    model = create_new_model()\n",
    "    optimizer = torch.optim.Adam(list(model.parameters()), lr=1e-4)\n",
    "    loss_fn_alex = lpips.LPIPS(net='vgg').to(device) # best forward scores\n",
    "    for epoch in range(2000):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        generated = model(x)\n",
    "\n",
    "        loss = loss_fn_alex(target, generated)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # if epoch % 100 == 0:\n",
    "        #     print(\"epoch %d, Total loss %0.6f\" % (epoch, loss))\n",
    "        #     fig, axes = plt.subplots(1,2, figsize=(12,6))\n",
    "        #     axes[0].imshow(tensor_to_numpy(target[0]))\n",
    "        #     axes[1].imshow(tensor_to_numpy(generated[0]))\n",
    "        #     plt.show()\n",
    "\n",
    "    psnr_dict[ind] = calculate_psnr(tensor_to_numpy(target[0]),tensor_to_numpy(generated[0]))\n",
    "    ssim_dict[ind] = calculate_ssim(tensor_to_numpy(target[0]),tensor_to_numpy(generated[0]))\n",
    "    print(psnr_dict[ind], ssim_dict[ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchsummary\n",
    "torchsummary.summary(model, (256,512,512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psnr_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssim_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psnr_total = 0\n",
    "for val in psnr_dict.values():\n",
    "    psnr_total += val\n",
    "psnr_average = psnr_total / len(psnr_dict)\n",
    "psnr_average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssim_total = 0\n",
    "for val in ssim_dict.values():\n",
    "    ssim_total += val\n",
    "\n",
    "ssim_average = ssim_total / len(ssim_dict)\n",
    "ssim_average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('baseline_lpips_vgg_psnr_dict.json', 'w') as fp:\n",
    "    json.dump(psnr_dict, fp)\n",
    "with open('baseline_lpips_vgg_ssim_dict.json', 'w') as fp:\n",
    "    json.dump(ssim_dict, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('fourier')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cff626a491088ff6eb8c947a8ca919a57363a7e5fa4daa621ee5be6aeffa2825"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
